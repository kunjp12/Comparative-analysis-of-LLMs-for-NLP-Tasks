{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmACPBdSVG5J"
   },
   "source": [
    "# Load Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J16r6eG-E_GC"
   },
   "outputs": [],
   "source": [
    "# Load Sentiment Analysis Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlR3zaJ0GgIK"
   },
   "outputs": [],
   "source": [
    "# Load the TSV file to check its contents\n",
    "tsv_file_path = '/content/en-annotated.tsv'\n",
    "\n",
    "# Read the first few lines to understand the structure of the file\n",
    "with open(tsv_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Display the first 5 lines to get a sense of the data\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIGlSBY3Ghwh"
   },
   "outputs": [],
   "source": [
    "#convert tsv file to csv\n",
    "\n",
    "import csv\n",
    "\n",
    "# Function to convert TSV to CSV\n",
    "def tsv_to_csv(tsv_filepath, csv_filepath):\n",
    "    with open(tsv_filepath, 'r', newline='', encoding='utf-8') as tsvfile, \\\n",
    "         open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "\n",
    "        tsv_reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        for row in tsv_reader:\n",
    "            # Ensure that text containing commas are quoted\n",
    "            row = ['\"' + field + '\"' if ',' in field else field for field in row]\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Convert the TSV file to CSV\n",
    "csv_file_path = '/content/en-annotated.csv'\n",
    "tsv_to_csv(tsv_file_path, csv_file_path)\n",
    "\n",
    "# Return the path to the new CSV file\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06Ik5EVGHg5M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/mnt/data/en-annotated.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data_info = data.info()\n",
    "data_head = data.head()\n",
    "data.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdIKGOKKHcdt"
   },
   "outputs": [],
   "source": [
    "# Since the dataset primarily consists of categorical data in the 'Category' column,\n",
    "# we will create a pie chart to visualize the proportion of top categories.\n",
    "\n",
    "# Selecting the top 10 categories for the pie chart\n",
    "top_categories = category_counts.head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(top_categories, labels=top_categories.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Top 10 Categories in the Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eenIJoexG5GK"
   },
   "outputs": [],
   "source": [
    "# remove \"\" from second column and store numbers as list\n",
    "# Function to modify the CSV: remove quotes and store numbers as list in the second column\n",
    "def modify_csv(input_csv_filepath, output_csv_filepath):\n",
    "    with open(input_csv_filepath, 'r', newline='', encoding='utf-8') as csvfile_in, \\\n",
    "         open(output_csv_filepath, 'w', newline='', encoding='utf-8') as csvfile_out:\n",
    "\n",
    "        csv_reader = csv.reader(csvfile_in)\n",
    "        csv_writer = csv.writer(csvfile_out)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if len(row) > 1:\n",
    "                # Remove quotes and convert the second column to a list of numbers\n",
    "                second_col = row[1].replace('\"', '').split(',')\n",
    "                second_col = [int(num.strip()) for num in second_col if num.strip().isdigit()]\n",
    "                row[1] = second_col\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Path for the modified CSV file\n",
    "modified_csv_file_path = '/content/en-annotated_modified.csv'\n",
    "\n",
    "# Modify the CSV file\n",
    "modify_csv(csv_file_path, modified_csv_file_path)\n",
    "\n",
    "# Return the path to the modified CSV file\n",
    "modified_csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDrxSCqOG_fE"
   },
   "outputs": [],
   "source": [
    "## create a new column and store these emotions anger:1, anticipation:2, disgust:3, fear:4, joy:5, sadness:6, surprise:7, trust:8, with neutral:0 as list according to the second column\n",
    "# Adjusted function to add a new column for emotions based on the second column's numbers\n",
    "def add_emotion_column_corrected(input_csv_filepath, output_csv_filepath, emotion_map):\n",
    "    with open(input_csv_filepath, 'r', newline='', encoding='utf-8') as csvfile_in, \\\n",
    "         open(output_csv_filepath, 'w', newline='', encoding='utf-8') as csvfile_out:\n",
    "\n",
    "        csv_reader = csv.reader(csvfile_in)\n",
    "        csv_writer = csv.writer(csvfile_out)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if len(row) > 1:\n",
    "                # Convert the string representation of the list back to an actual list\n",
    "                num_list = eval(row[1])\n",
    "                # Map the numbers to emotions\n",
    "                emotion_list = [emotion_map[num] for num in num_list]\n",
    "            else:\n",
    "                emotion_list = []\n",
    "            # Append the emotion list as a new column\n",
    "            row.append(emotion_list)\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "# Path for the CSV file with the corrected emotion column\n",
    "csv_with_emotion_corrected_file_path = '/content/en-annotated_with_emotions_corrected.csv'\n",
    "\n",
    "emotions_mapping = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust',\n",
    "                    'neutral']\n",
    "\n",
    "# Add the emotion column to the CSV file using the corrected function\n",
    "add_emotion_column_corrected(modified_csv_file_path, csv_with_emotion_corrected_file_path, emotions_mapping)\n",
    "\n",
    "# Return the path to the new CSV file with emotions\n",
    "csv_with_emotion_corrected_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0S3M69nE-hE"
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_noise_and_save(input_csv_filepath, output_csv_filepath):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('/content/en-annotated_with_emotions_corrected.csv')\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Optionally, add any specific filters here if needed\n",
    "    # Example: df = df[df['some_column'] != 'some_value']\n",
    "\n",
    "    # Save the cleaned dataset\n",
    "    df.to_csv(output_csv_filepath, index=False)\n",
    "\n",
    "# Path to the CSV file after adding emotion column\n",
    "csv_with_emotion_file_path = '/content/en-annotated_with_emotions_corrected.csv'\n",
    "\n",
    "# Path for the cleaned dataset\n",
    "cleaned_csv_file_path = '/content/filtered_sentiment_analysis_dataset.csv'\n",
    "\n",
    "# Remove noise and save the dataset\n",
    "remove_noise_and_save(csv_with_emotion_file_path, cleaned_csv_file_path)\n",
    "\n",
    "# Return the path to the cleaned dataset\n",
    "cleaned_csv_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi2rUvm4VDAa"
   },
   "source": [
    "# Load QA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYjXL9JDDAyK"
   },
   "outputs": [],
   "source": [
    "# Load Question answer dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGE5wn-dD9Be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Basic statistics and visualization for the dataset\n",
    "\n",
    "# Count of non-null values in each column\n",
    "nonnull_counts = data.notnull().sum()\n",
    "\n",
    "# Plotting the count of non-null values for each column\n",
    "plt.figure(figsize=(15, 6))\n",
    "nonnull_counts.plot(kind='bar')\n",
    "plt.title('Count of Non-Null Values per Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Non-Null Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Saving the plot\n",
    "nonnull_plot_path = 'nonnull_counts_plot.png'\n",
    "plt.savefig(nonnull_plot_path)\n",
    "\n",
    "# Plotting the number of unique values per column (excluding columns with very few non-null values)\n",
    "unique_counts = data.nunique()\n",
    "unique_counts_filtered = unique_counts[unique_counts > 1]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "unique_counts_filtered.plot(kind='bar')\n",
    "plt.title('Number of Unique Values per Column (Filtered)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Unique Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Saving the plot\n",
    "unique_counts_plot_path = 'unique_counts_plot.png'\n",
    "plt.savefig(unique_counts_plot_path)\n",
    "\n",
    "nonnull_plot_path, unique_counts_plot_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPY8zx_iETh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = train_test_split(data, test_size=0.20, random_state=42)\n",
    "\n",
    "# Display the first few rows of the training and testing sets\n",
    "train_data.head(), test_data.head(), train_data.shape, test_data.shape\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
